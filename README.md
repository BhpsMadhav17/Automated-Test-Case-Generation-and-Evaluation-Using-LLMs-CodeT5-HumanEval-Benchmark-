This project explores the effectiveness of Large Language Models (LLMs), particularly CodeT5, in automatically generating software test cases. 
We compare the LLM-generated test cases with human-written ones using the HumanEval benchmark, evaluate them using pass@k metrics, and visualize performance through various statistical techniques.

